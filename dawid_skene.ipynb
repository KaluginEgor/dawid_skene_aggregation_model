{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dawid, Skene, 1979\n",
        "\n",
        "We have $n_{ik}^u$ &mdash; the number of times the worker $u \\in U$ set class $k \\in K$ to object $i \\in I$ (perhaps the worker saw this object several times). Let us denote by $Y_{ik} = I\\{\\text{object $i$ of class $k$}\\}$, these are our latent variables.\n",
        "\n",
        "Parameters we have\n",
        "* $\\pi_{k\\ell}^u$ &mdash; the probability that the worker $u$ set class $\\ell$ instead of the correct class $k$. \n",
        "* $\\rho_k$ &mdash; probability of class $k$.\n",
        "\n",
        "\n",
        "$$p_{\\pi,p}(N, Y) = \\prod_{i\\in I}p(N_i, Y_i),$$\n",
        "\n",
        "If $k$ is the number of the class of the $i$th object, then\n",
        "\n",
        "$$p(N_i, Y_i)=\\underbrace{p(\\text{object $i$ belonging to class $k$})}_{=\\rho_k}p(N_i\\mid\\text{object $i$ belonging to class $k$})$$\n",
        "\n",
        "(the values of $Y_{it}$ are determined by the number of the true class, so $Y_i$ disappears on the right). Further, we assume that the workers act independently, so\n",
        "\n",
        "$$p(N_i\\mid\\text{object $i$ belonging to class $k$}) = \\prod_{u\\in U}p(N_i^u\\mid\\text{object $i$ belonging to class $k$}).$$\n",
        "\n",
        "Let's look at the value $p(N_i^u\\mid\\text{object $i$ belonging to class $k$})$. It is responsible for the classes $u$-th worker has set to $i$-object. We assume that the worker's encounters with the object are ordered by time, then\n",
        "\n",
        "$$p(\\text{$u$th worker assigned the $i$th object to classes $k'_1,\\ldots,k'_r$}\\mid\\text{object $i$ belonging to class $k$}) =$$\n",
        "\n",
        "$$=\\prod_{s}p(\\text{at the $s$-th encounter with the $i$-th object, the $u$-th worker assigned it to the class $k'_s$}\\mid\\text{object $i$ belonging to class $k$})$$\n",
        "\n",
        "This probability can be rewritten as\n",
        "\n",
        "$$\\prod_{\\ell \\in K} \\left( \\pi_{k\\ell}^u \\right)^{n_{i\\ell}^u},$$\n",
        "\n",
        "and the final likelihood appears as\n",
        "\n",
        "$$p_{\\pi,p}(N, Y) = \\prod_{i\\in I}\\prod_{k \\in K} \\left( \\rho_k \\prod_{u\\in U} \\prod_{\\ell \\in K} \\left( \\pi_{k\\ell}^u \\right)^{n_{i\\ell}^u} \\right)^{Y_{ik}}$$\n",
        "\n",
        "We need to maximize it by $\\pi$ and $\\rho$\n",
        "\n",
        "\n",
        "**E-step:**\n",
        "\n",
        "$q(Y) = p_{\\pi,p}(Y | N) \\propto p_{\\pi,p}(N, Y) = \\prod_{i\\in I} p_{\\pi,p}(N_i, Y_i) \\Rightarrow q(Y) = \\prod_{i\\in I} q_i(Y_i)$\n",
        "\n",
        "$q_i(Y_i) \\propto p_{\\pi,p}(N_i, Y_i) = \\prod_{k \\in K} \\left( \\rho_k \\prod_{u\\in U} \\prod_{\\ell \\in K} \\left( \\pi_{k\\ell}^u \\right)^{n_{i\\ell}^u} \\right)^{Y_{ik}}$\n",
        "\n",
        "$q_i(j) \\propto  \\rho_j \\prod_{u\\in U} \\prod_{\\ell \\in K} \\left( \\pi_{j\\ell}^u \\right)^{n_{i\\ell}^u} \\Rightarrow q_i(j) = \\dfrac{\\rho_j \\prod_{u\\in U} \\prod_{\\ell \\in K} \\left( \\pi_{j\\ell}^u \\right)^{n_{i\\ell}^u}}{\\sum_{q=1}^{K} \\rho_q \\prod_{u\\in U} \\prod_{\\ell \\in K} \\left( \\pi_{q\\ell}^u \\right)^{n_{i\\ell}^u}} = \\gamma_{ij}$\n",
        "\n",
        "**M-step**\n",
        "\n",
        "$\\mathbb{E}_q \\log p_{\\pi,p}(N, Y) = \\sum_{i\\in I}\\sum_{k \\in K} \\mathbb{E}_q Y_{ik} \\left( \\log \\rho_k + \\sum_{u\\in U} \\sum_{\\ell \\in K} n_{i\\ell}^u \\log \\pi_{k\\ell}^u \\right)$\n",
        "\n",
        "$\\mathbb{E}_q \\log p_{\\pi,p}(N, Y) \\rightarrow \\max_{\\rho}$\n",
        "\n",
        "$\n",
        "\\begin{cases}\n",
        "  \\sum_{i\\in I}\\sum_{k \\in K} \\gamma_{ik} \\log \\rho_k \\rightarrow \\max_{\\rho}\\\\\n",
        "  \\sum_{k \\in K} \\rho_k = 1; \\: \\rho_k \\ge 0\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "Lagrangian:\n",
        "\n",
        "$\\mathcal{L} = \\sum_{i\\in I}\\sum_{k \\in K} \\gamma_{ik} \\log \\rho_k - \\lambda \\left(\\sum_{k \\in K} \\rho_k - 1 \\right)$\n",
        "\n",
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial \\rho_k} = \\sum_{i\\in I} \\dfrac{\\gamma_{ik}}{\\rho_k} - \\lambda = 0 \\Rightarrow \\\\ \\hat{\\rho_k} = \\dfrac{\\sum_{i\\in I} \\gamma_{ik}}{\\lambda} = \\dfrac{\\sum_{i\\in I} \\gamma_{ik}}{\\sum_{i\\in I} \\sum_{k \\in K} \\gamma_{ik}} = \\dfrac{\\sum_{i\\in I} \\gamma_{ik}}{|I|}$\n",
        "\n",
        "$\\mathbb{E}_q \\log p_{\\pi,p}(N, Y) \\rightarrow \\max_{\\pi}$\n",
        "\n",
        "$\n",
        "\\begin{cases}\n",
        "  \\sum_{i\\in I}\\sum_{k \\in K} \\gamma_{ik} \\sum_{u\\in U} \\sum_{\\ell \\in K} n_{i\\ell}^u \\log \\pi_{k\\ell}^u \\rightarrow \\max_{\\pi} \\\\\n",
        "  \\sum_{\\ell \\in K} \\pi_{k\\ell}^u = 1; \\: \\pi_{k\\ell}^u \\ge 0\n",
        "\\end{cases}\n",
        "$\n",
        "\n",
        "Lagrangian:\n",
        "\n",
        "$\\mathcal{L} = \\sum_{i\\in I}\\sum_{k \\in K} \\gamma_{ik} \\sum_{u\\in U} \\sum_{\\ell \\in K} n_{i\\ell}^u \\log \\pi_{k\\ell}^u - \\sum_{k \\in K} \\sum_{u\\in U} \\lambda_{k}^{u} \\left(\\sum_{\\ell \\in K} \\pi_{k\\ell}^u - 1 \\right)$\n",
        "\n",
        "$\\dfrac{\\partial \\mathcal{L}}{\\partial \\pi_{k\\ell}^u} = \\sum_{i\\in I} \\gamma_{ik} \\dfrac{n_{i\\ell}^u}{\\pi_{k\\ell}^u} - \\lambda_k^u = 0 \\Rightarrow \\\\ \\hat{\\pi}_{k\\ell}^u = \\dfrac{\\sum_{i\\in I} \\gamma_{ik} n_{i \\ell}^u}{\\lambda_k^u} = \\dfrac{\\sum_{i\\in I} \\gamma_{ik} n_{i \\ell}^u}{\\sum_{i\\in I} \\sum_{\\ell \\in K} \\gamma_{ik} n_{i \\ell}^u}$"
      ],
      "metadata": {
        "id": "rt6bzAZDzFxc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jVbqryuyPmW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "_EPS = np.float_power(10, -10)\n",
        "\n",
        "class DawidSkene:\n",
        "\n",
        "    def __init__(self, n_iter = 100, tolerance = 0.01):\n",
        "        self.n_iter = n_iter\n",
        "        self.tolerance = tolerance\n",
        "    \n",
        "    def _e_step(self, data, priors, errors):\n",
        "        joined = data.join(np.log2(errors), on=['worker', 'label'])\n",
        "        joined.drop(columns=['worker', 'label'], inplace=True)\n",
        "        log_likelihoods = np.log2(priors) + joined.groupby('task', sort=False).sum()\n",
        "        log_likelihoods.rename_axis('label', axis=1, inplace=True)\n",
        "        scaled_likelihoods = np.exp2(log_likelihoods.sub(log_likelihoods.max(axis=1), axis=0))\n",
        "        return scaled_likelihoods.div(scaled_likelihoods.sum(axis=1), axis=0)\n",
        "    \n",
        "    def _m_step(self, data, probas):\n",
        "        joined = data.join(probas, on='task')\n",
        "        joined.drop(columns=['task'], inplace=True)\n",
        "\n",
        "        errors = joined.groupby(['worker', 'label'], sort=False).sum()\n",
        "        errors.clip(lower=_EPS, inplace=True)\n",
        "        errors /= errors.groupby('worker', sort=False).sum()\n",
        "\n",
        "        return errors\n",
        "    \n",
        "    def _evidence_lower_bound(self, data, probas, priors, errors):\n",
        "        joined = data.join(np.log(errors), on=['worker', 'label'])\n",
        "\n",
        "        joined = joined.rename(columns={True: 'True', False: 'False'}, copy=False)\n",
        "        priors = priors.rename(index={True: 'True', False: 'False'}, copy=False)\n",
        "\n",
        "        joined.loc[:, priors.index] = joined.loc[:, priors.index].add(np.log(priors))\n",
        "\n",
        "        joined.set_index(['task', 'worker'], inplace=True)\n",
        "        joint_expectation = (probas.rename(columns={True: 'True', False: 'False'}) * joined).sum().sum()\n",
        "\n",
        "        entropy = -(np.log(probas) * probas).sum().sum()\n",
        "        return float(joint_expectation + entropy)\n",
        "\n",
        "    def fit(self, data):\n",
        "        data = data[['task', 'worker', 'label']]\n",
        "\n",
        "        scores = data[['task', 'label']].value_counts().unstack('label', fill_value=0)\n",
        "        probas = scores.div(scores.sum(axis=1), axis=0)\n",
        "        priors = probas.mean()\n",
        "        errors = self._m_step(data, probas)\n",
        "        loss = -np.inf\n",
        "        self.loss_history_ = []\n",
        "\n",
        "        for _ in range(self.n_iter):\n",
        "            probas = self._e_step(data, priors, errors)\n",
        "            priors = probas.mean()\n",
        "            errors = self._m_step(data, probas)\n",
        "            new_loss = self._evidence_lower_bound(data, probas, priors, errors) / len(data)\n",
        "            self.loss_history_.append(new_loss)\n",
        "\n",
        "            if new_loss - loss < self.tolerance:\n",
        "                break\n",
        "            loss = new_loss\n",
        "\n",
        "        probas.columns = pd.Index(probas.columns, name='label', dtype=probas.columns.dtype)\n",
        "\n",
        "        self.probas_ = probas\n",
        "        self.priors_ = priors\n",
        "        self.errors_ = errors\n",
        "        self.labels_ = probas.idxmax(axis='columns')\n",
        "\n",
        "        return self\n",
        "    \n",
        "    def fit_predict_proba(self, data):\n",
        "        return self.fit(data).probas_\n",
        "\n",
        "    def fit_predict(self, data):\n",
        "        return self.fit(data).labels_"
      ]
    }
  ]
}